model_name: CatBoostModel

cv_method: StratifiedKFold
n_splits: 5
shuffle: true
metric: AUC

# https://catboost.ai/en/docs/references/training-parameters/common
# https://knknkn.hatenablog.com/entry/2021/06/29/125226
# https://nykergoto.hatenablog.jp/entry/2019/03/29/%E5%8B%BE%E9%85%8D%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%B0%97%E6%8C%81%E3%81%A1
params:
  loss_function: Logloss
  eval_metric: AUC
  learning_rate: 0.05  # 木を足し合わせるときに使う重み係数. 大きいと各木の情報を多く使い使用する木の数(num_iterations)が減り荒いモデルになる. 小さいと各木の情報を少なく使い使用する木の数(num_iterations)が増え滑らかなモデルになる. 基本小さい程精度良いが時間とのトレードオフ. チューニング不要
  max_depth: 5  # 木の深さ. 通常は3-8くらい. defaultは-1で制限なしとなり過学習になりやすい.
  num_leaves: 31  # 葉の最大数. 理論上2^(max_depth)未満にしかならない. 大きくすると複雑なモデルになるが過学習しやすくなる.
  min_data_in_leaf: 66  # 1つの葉に入る最小データ数. 値が小さいと葉が細かく分割されるので, 複雑なモデルになるが過学習しやすくなる.
  # colsample_bylevel: 0.8  # 1つの木を作成する際に用いる特徴量(column)の割合. なお各木で用いる特徴量はランダムに選ばれる.
  subsample: 0.8  # 1つの木を作成する際に用いるデータ(row)の割合. なお各木で用いるデータはランダムに選ばれる.
  l2_leaf_reg: 1  # L2正則化項の係数
  random_seed: 42  # seed値
  num_boost_round: 1000000  # 作成する木の数. early_stoppingを使うので基本無限大. チューニング不要. n_estimatorsと同じ.
  task_type: CPU  # CPU or GPU
  allow_writing_files: false  # catboost_infoファイルを作成するか否か

early_stopping_rounds: 100
verbose: 100

features:
  - IdentityBlock
  - LabelEncodingBlock
  - CountEncodingBlock
  - GroupbyBlock
  - TargetEncodingBlock

categorical_features:
  - "le_Pclass"
  - "le_Name"
  - "le_Sex"
  - "le_Ticket"
  - "le_Cabin"
  - "le_Embarked"
